<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Haxin Mainframes]]></title>
  <link href="http://bbarrows.github.com/atom.xml" rel="self"/>
  <link href="http://bbarrows.github.com/"/>
  <updated>2014-11-05T14:12:14-08:00</updated>
  <id>http://bbarrows.github.com/</id>
  <author>
    <name><![CDATA[Brad Barrows]]></name>
    <email><![CDATA[bradleyb1537@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Thinkpad W530 Nvidia Divers]]></title>
    <link href="http://bbarrows.github.com/blog/2014/11/05/ThinkPad-w530-nVidia-Divers/"/>
    <updated>2014-11-05T00:00:00-08:00</updated>
    <id>http://bbarrows.github.com/blog/2014/11/05/ThinkPad-w530-nVidia-Divers</id>
    <content type="html"><![CDATA[<pre><code>---
</code></pre>

<p>layout: post
title: &ldquo;Using nVidia Drivers on a Thinkpad w530&rdquo;
date: 2014-11-5 12:47
comments: true</p>

<h2>categories: [Ubuntu, Thinkpad, w530, nVidia, Drivers]</h2>

<h2>Drivers</h2>

<p>To get the nVidia Drivers working with a w530 laptop you must select the Discrete Video Driver in the BIOS.</p>

<p>Then run the following to install the ?latest? (using sudo apt-get install nvidia-current may be better) nvidia drivers:</p>

<pre><code>sudo add-apt-repository -y ppa:ubuntu-x-swat/x-updates; sudo apt-get update; sudo apt-get install nvidia-331
</code></pre>

<p>The driver install should automatically create a new xorg.conf but if not you can run:</p>

<pre><code>sudo nvidia-xconfig
</code></pre>

<p>Some people even chose to purge the previous integrated drivers:</p>

<pre><code>sudo apt-get purge xserver-xorg-video-nouveau
</code></pre>

<p>Add blacklist it from the kernel (if there is no blacklist-nouveau.conf in /etc/modprobe.d/ you can just create it):</p>

<pre><code>sudo vim /etc/modprobe.d/blacklist-nouveau.conf
</code></pre>

<p>Adding these lines:</p>

<pre><code>blacklist nouveau
blacklist lbm-nouveau
options nouveau modeset=0
alias nouveau off
alias lbm-nouveau off
</code></pre>

<p>(A StackOverflow post also shows another way)[<a href="http://askubuntu.com/questions/451221/ubuntu-14-04-install-nvidia-driver">http://askubuntu.com/questions/451221/ubuntu-14-04-install-nvidia-driver</a>] to disable/blacklist the driver:</p>

<pre><code>sudo echo options nouveau modeset=0 | sudo tee -a /etc/modprobe.d/nouveau-kms.conf
sudo update-initramfs -u
</code></pre>

<p>This worked great until I had a system update. I rebooted and my laptop was flashing, trying to “startx” but failing over and over, could not even switch to a terminal via ctr-alt-f1</p>

<p>It sounds like this can be fixed by (rebuilding with every kernel update)[<a href="http://askubuntu.com/questions/536562/ubuntu-14-04-with-nvidia-driver-blank-screen-after-kernel-update">http://askubuntu.com/questions/536562/ubuntu-14-04-with-nvidia-driver-blank-screen-after-kernel-update</a>]:</p>

<pre><code>sudo apt-get install dkms build-essential linux-headers-generic linux-headers-`uname -r` linux-source
</code></pre>

<p>It will install dkms and the headers before you run the installer and it should give you a DKMS option during setup. DKMS will prevent the problem you are experiencing so you don&rsquo;t have to re-install every kernel upgrade.</p>

<p>The issue is that every kernel upgrade the nVidia drivers are not rebuild/configured with the kernel upgrade..</p>

<h2>GRUB</h2>

<p>You will also need to update your boot loader, adding a boot option, nox2apic:</p>

<pre><code>sudo vim /etc/default/grub
</code></pre>

<p>and add the &ldquo;nox2apic&rdquo; flag to the GRUB_CMDLINE_LINUX option, or in my case it was the GRUB_CMDLINE_LINUX_DEFAULT. I would look for the variable with the nosplash option and add it to that one, I am sure it would not hurt to add nox2apic to both if you do have both the GRUB_CMDLINE_LINUX and GRUB_CMDLINE_LINUX_DEFAULT options in ur grub config.</p>

<p>You will be changing something like:
    GRUB_CMDLINE_LINUX_DEFAULT=&ldquo;quiet splash&rdquo;
to:
    GRUB_CMDLINE_LINUX_DEFAULT=&ldquo;quiet splash nox2apic&rdquo;</p>

<p>To update grub you will need to run:</p>

<pre><code>sudo update-grub
</code></pre>

<p><a href="http://blog.jordanengbers.com/2012/11/nvidia-optimus-and-multiple-monitors-in-ubunt/">This Site</a> has a breakdown on why this option in needed.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Checking Out Sublime Text 3 Binary Hex Diff Versus Cracked Version]]></title>
    <link href="http://bbarrows.github.com/blog/2014/09/11/checking-out-sublime-text-3-binary-hex-diff-versus-cracked-version/"/>
    <updated>2014-09-11T09:55:14-07:00</updated>
    <id>http://bbarrows.github.com/blog/2014/09/11/checking-out-sublime-text-3-binary-hex-diff-versus-cracked-version</id>
    <content type="html"><![CDATA[<p>I saw a site <a href="http://ict-tricks.blogspot.com/2014/09/how-to-craked-sublime-text-3-in-ubuntu.html">Sublime Text 3 with hack/crack</a> and I am always paranoid. Wondering what the actual difference was between the
3056 build and the crack I checked.</p>

<p>Running:</p>

<pre><code>xxd sublime_text\ crack\ linux\ 64\ build\ 3065 c1.hex
xxd /opt/sublime_text/sublime_text c2.hex
diff c1.hex c2.hex
</code></pre>

<p>I saw that there was the following diff:</p>

<pre><code>2111c2111
&lt; 00083e0: f88e 00e8 b751 0700 3bc0 0f94 c084 c088  .....Q..;.......
---
&gt; 00083e0: f88e 00e8 b751 0700 85c0 0f94 c084 c088  .....Q..........
</code></pre>

<p>Putting that into an <a href="http://www.onlinedisassembler.com/odaweb/">online assembler/disassembler</a>:</p>

<p>The original file:</p>

<pre><code>.data:0x00000000    f8  clc
.data:0x00000001    8e00    mov    es,WORD PTR [rax]
.data:0x00000003    e8b7510700  call   func_000751bf
.data:0x00000008    3bc0    cmp    eax,eax
.data:0x0000000a    0f94c0  sete   al
.data:0x0000000d    84c0    test   al,al
</code></pre>

<p>The cracked file:</p>

<pre><code>.data:0x00000000    f8  clc
.data:0x00000001    8e00    mov    es,WORD PTR [rax]
.data:0x00000003    e8b7510700  call   func_000751bf
.data:0x00000008    85c0    test   eax,eax
.data:0x0000000a    0f94c0  sete   al
.data:0x0000000d    84c0    test   al,al
</code></pre>

<p>Notice the only different is the cmp to test. According to an assembly reference I see that test is just a bitwise AND comparison so test eax,eax will always AND the same values always having the same result.</p>

<p>Looks like a safe hack to me..</p>

<p>NOTE: I pay for Sublime Text 3. It is amazing software and I am all for supporting the authors. This page just came up when searching for Sublime Text 3 and I was curious.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Openstack Rdo Centos Error]]></title>
    <link href="http://bbarrows.github.com/blog/2014/09/10/Openstack-RDO-CentOS-Error/"/>
    <updated>2014-09-10T00:00:00-07:00</updated>
    <id>http://bbarrows.github.com/blog/2014/09/10/Openstack-RDO-CentOS-Error</id>
    <content type="html"><![CDATA[<pre><code>---
</code></pre>

<p>layout: post
title: &ldquo;Installing Openstack via RDO on CentOS 6.5&rdquo;
date: 2014-09-10 16:47
comments: true</p>

<h2>categories: [Python, Openstack, Virtualization, CentOS, Work]</h2>

<h2>RDO Packstack failing to install Openstack with the latest CentOS</h2>

<p>Today I needed to bring up a new &ldquo;all in one&rdquo; Openstack Virtual Machine</p>

<p>I now work at a company called Virtustream. Here I am currently working on projects that require to me to run a few virtual machines at a time all on my poor little W530 Thinkpad laptop. The strain (or most likely the heat from me being an idiot and working from bed when sick which did not allow my laptop to breath well probably) on my laptop caused my HardDrive to crash.</p>

<p>Went to get to work in the morning and was greeted by my favorite:</p>

<pre><code>Cannot run anything, your filesystem is mounted read only, etc etc messages..
</code></pre>

<p>I have seen this a few times before. Replace my hard drive maybe 3 or 4 times already (I bought a laptop pad with ventilation in case I work from bed anymore). Anyhow I just upgraded to a SSD HD. I do not know how I ever worked without this thing.. It is such an upgrade.
So much faster.</p>

<p>Anyway, so what this whole blog post is really about is when I went to install my IceHouse Openstack All In One via RedHat&rsquo;s RDO PackStack, which is found at <a href="https://openstack.redhat.com/Quickstart">RDO PackStack Quick Install</a>. I ran through the simple directions like usual but the install failed. I even tried disabling SELinux by editing: /etc/selinux/config and setting:</p>

<pre><code>SELINUX=permissive
</code></pre>

<p> then restarting and installing. I tried it with a fresh VM then, re installed Cent OS 6.5. Nothing would work.</p>

<p> It kept dying at:</p>

<pre><code>192.168.122.166_nova.pp:                             [ DONE ]
Applying 192.168.122.166_neutron.pp
192.168.122.166_neutron.pp:                       [ ERROR ]
Applying Puppet manifests                         [ ERROR ]
</code></pre>

<p>The logs shows the following in red:</p>

<pre><code>Warning: Config file /etc/puppet/hiera.yaml not found, using Hiera defaults
Warning: Scope(Class[Neutron::Server]): The sql_connection parameter is deprecated, use database_connection instead.
Warning: Scope(Class[Neutron::Plugins::Ml2]): enable_security_group is deprecated. Security is managed by the firewall_drive value in ::neutron::agents::ml2::ovs.


Warning: The package type's allow_virtual parameter will be changing its default value from false to true in a future release. If you do not want to allow virtual packages, please explicitly set allow_virtual to false.
   (at /usr/lib/ruby/site_ruby/1.8/puppet/type/package.rb:430:in `default')


Error: sysctl -p /etc/sysctl.conf returned 255 instead of one of [0]
Error: /Stage[main]/Packstack::Neutron::Bridge/Exec[sysctl_refresh]/returns: change from notrun to 0 failed: sysctl -p /etc/sysctl.conf returned 255 instead of one of [0]
</code></pre>

<p>Among a bunch of normal colored notices..</p>

<p>After googling for a while I found that someone had posted a bug and fix!!</p>

<p><a href="https://bugzilla.redhat.com/show_bug.cgi?format=multiple&amp;id=1132129">Openstack PackStack CentOS 6.5 Install Patch</a></p>

<p>Looking through the patch for the fix I see that they just needed to add a -e parameter to sysctl to ignore some new keys added to /etc/sysctl.conf in CentOS 6.5</p>

<hr />

<p>Finally, the workaround. Well you do not need to re format/start form scratch like I did a few times. You can just go and edit:</p>

<pre><code>vim /usr/share/openstack-puppet/modules/packstack/manifests/neutron/bridge.pp
</code></pre>

<p>And then change the line:</p>

<pre><code>    command =&gt; 'sysctl -p /etc/sysctl.conf',
</code></pre>

<p>to:</p>

<pre><code>    command =&gt; 'sysctl -e -p /etc/sysctl.conf',
</code></pre>

<p>Note the new -e param. This ignores unknown keys so that no error will be returned, crashing the packstack install.</p>

<p>Then to continue your packstack install, you need to re run packstack but with your &ldquo;answer file&rdquo; which contains all the passwords and info needed from the partially complete install you just tried to do:</p>

<p>Run this via:</p>

<pre><code>packstack --answer-file=packstack-answers-20140910-111306.txt
</code></pre>

<p>NOTE: You answer file with have a different name/timestamp!</p>

<p>And after this your install should work!  Good luck!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to Record Live Video From PyCon]]></title>
    <link href="http://bbarrows.github.com/blog/2013/03/15/pycon/"/>
    <updated>2013-03-15T12:30:00-07:00</updated>
    <id>http://bbarrows.github.com/blog/2013/03/15/pycon</id>
    <content type="html"><![CDATA[<p>PyCon has so many awesome talks! The problem is that they are happening at the same time!</p>

<p>To deal with this I have a 2 part solution:</p>

<ul>
<li>First I goto a page I want to watch and run this JS code I whipped up to grab the actual video URLS:</li>
</ul>


<p>JS Code:</p>

<pre><code>for (m in player_jwobject.config.modes) {
    console.log(player_jwobject.config.modes[m].type);
    if (player_jwobject.config.modes[m].config.levels) {
        for (l in player_jwobject.config.modes[m].config.levels) {
            console.log(" - " + player_jwobject.config.modes[m].config.levels[l].file)
        }
    }
};
</code></pre>

<p>Note that this code gives you both html5 and flash options for streaming video. And will look like this:</p>

<pre><code>html5
 - http://50.16.83.230:8080/webcast-low.webm?q=1363375116439
 - http://50.16.83.230:8080/webcast-high.webm?q=1363375116439
flash
 - http://50.16.83.230:8080/webcast-low.flv?q=1363375116439
 - http://50.16.83.230:8080/webcast-high.flv?q=1363375116439
download
</code></pre>

<ul>
<li>Chose and dump one of these streams. Either will probably work fine however I chose to low quality html5 stream:</li>
</ul>


<p>Command to run (requires mplayer):</p>

<pre><code>mplayer -dumpstream "http://50.16.83.230:8080/webcast-low.webm?q=1363375116439"\
 -dumpfile interpetermplayer
</code></pre>

<p>Or use ffmpeg:</p>

<pre><code>ffmpeg -i http://50.16.160.194:8080/webcast-high.webm\?q\=1363381000621 pycon.webm
</code></pre>

<p>This will save the webm stream to a file called: interpetermplayer</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Ubuntu(Kubuntu) Web Pages Load Painfully Slow]]></title>
    <link href="http://bbarrows.github.com/blog/2013/02/12/ubuntu1210/"/>
    <updated>2013-02-12T22:30:00-08:00</updated>
    <id>http://bbarrows.github.com/blog/2013/02/12/ubuntu1210</id>
    <content type="html"><![CDATA[<p>My new Laptop from work, I recently started a new job at a company called Virtustream, was loaded with Ubuntu (Kubuntu I believe actually since its running KDE and I do not think that is the default with Ubuntu?) which was fine, I like the aptitude package manager, however I noticed that web pages loaded PAINFULLY SLOW. I tried anumber of things but when I saw that it was really new sites that I hadn&rsquo;t yet visited that were loading slowly I guessed DNS.</p>

<p>Upon googling Ubuntu 12.10 DNS I saw there were a number of people complaining. I decided to give it a try and set my /etc/resolv.conf to Google&rsquo;s DNS servers:</p>

<pre><code>nameserver 8.8.8.8
nameserver 8.8.4.4
</code></pre>

<p>And now it runs like charm..</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using UDP in the Python Tornado Framework]]></title>
    <link href="http://bbarrows.github.com/blog/2013/01/27/udptornado/"/>
    <updated>2013-01-27T12:48:00-08:00</updated>
    <id>http://bbarrows.github.com/blog/2013/01/27/udptornado</id>
    <content type="html"><![CDATA[<p>A little while ago I was working on an API endpoint that needed to ask the BitTorrent Live video streaming trackers how many people were watching what swarms. I needed to do this by sending the byte 4 to the tracker on a certain ip and port. We were using Tornado. Previously to use UDP sockets with the Tornado event loop (in my python DHT project for example) I just created a non blocking UDP socket and added a handler for the READ state.</p>

<pre><code>self.sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
self.io_loop.add_handler(self.sock.fileno(), self.handle_input, self.io_loop.READ)

udpsock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
udpsock.setblocking(0)
</code></pre>

<p>The callback for this handler looked like this:</p>

<pre><code>def handle_input(self, fd, events):
    (data, source_ip_port) = self.sock.recvfrom(4096)
    bdict = bdecode(data)

    #Got a response from some previous query
    if bdict["y"] == "r":
        self.handle_response(bdict, source_ip_port)

    #Porb gonna have to ad a listenr socket
    #Got a query for something
    if bdict["y"] == "q":
        self.handle_query(bdict, source_ip_port)
</code></pre>

<p>I believe this is an appropriate way to handle UDP sockets in Tornado (as the library only comes with TCP/HTTP based clients I know of..). However my friend <a href="http://kyle.graehl.org/">Kyle Grahel</a> put together a nice <a href="http://kyle.graehl.org/coding/2012/12/07/tornado-udpstream.html">UDP Wrapper</a> class that is much closer to and even takes methods from the general Tornado IOStream class.</p>

<p>Using the UDPWrapper I was able to do something like this (I actually added an enter and exit for the with however I am not sure if its actually very pythonic to do it that way versus try finally):</p>

<pre><code>udpsockwrapper = UDPSockWrapper(udpsock, in_ioloop=io_loop)
response = None
with udpsockwrapper:
    udpsockwrapper.sendto(chr(4), (tracker_ip, int(tracker_port)))
    response = yield gen.Task(udpsockwrapper.read_chunk)
</code></pre>

<p>You may notice the yield gen.Task above? This is using Tornado&rsquo;s awesome gen library. It basically allows you to turn your functions into generators which the event loop basically iterates through as it hits your callbacks. This allows you to take your nested callback code and turn it into a synchronous style. I believe this is similar to the Deferred class you yield with in the Twisted framework.</p>

<p>My modified version of the UDPWrapper:</p>

<pre><code>import tornado, time

#From Kyle Grahel - http://kyle.graehl.org/
#The __enter__ and __exit__ are added by me.. probably not the best way to use
#these though..
class UDPSockWrapper(object):
    def __enter__(self):
        return

    def __exit__(self, type, value, traceback):
        self.close()

    def __init__(self, socket, in_ioloop=None):
        self.socket = socket
        self._state = None
        self._read_callback = None
        self.ioloop = in_ioloop or tornado.ioloop.IOLoop.instance()

    def __repr__(self):
        return "&lt;UDPSockWrap:%s,rc:%s&gt;" % (self.socket.fileno(), self._read_callback)

    def _add_io_state(self, state):
        if self._state is None:
            self._state = tornado.ioloop.IOLoop.ERROR | state
            #with stack_context.NullContext():
            self.ioloop.add_handler(
                self.socket.fileno(), self._handle_events, self._state)
        elif not self._state &amp; state:
            self._state = self._state | state
            self.ioloop.update_handler(self.socket.fileno(), self._state)

    def sendto(self, msg, dest):
        return self.socket.sendto(msg, dest)

    def recv(self,sz):
        return self.socket.recv(sz)

    def close(self):
        self.ioloop.remove_handler(self.socket.fileno())
        self.socket.close()
        self.socket = None

    def read_chunk(self, callback=None, timeout=4):
        self._read_callback = callback
        self._read_timeout = self.ioloop.add_timeout( time.time() + timeout, 
            self.check_read_callback )
        self._add_io_state(self.ioloop.READ)

    def check_read_callback(self):
        if self._read_callback:
            # XXX close socket?
            #data = self.socket.recv(4096)
            self._read_callback(None, error='timeout')

    def _handle_read(self):
        if self._read_timeout:
            self.ioloop.remove_timeout(self._read_timeout)
        if self._read_callback:
            try:
                data = self.socket.recv(4096)
            except:
                # conn refused??
                data = None
            self._read_callback(data);
            self._read_callback = None

    def _handle_events(self, fd, events):
        if events &amp; self.ioloop.READ:
            self._handle_read()
        if events &amp; self.ioloop.ERROR:
            logging.error('%s event error' % self)
</code></pre>

<p>Another side note is that read_chunk above has the keyword argument callback. This is a requirement for the gen.Task class. The function that it executes should have a keyword argument callback=None. In order to convert any function to a function with this callback kwarg I used this lambda:</p>

<pre><code>lambda **kwargs: db.get_item('users', {"HashKeyElement": {"S": username}}, kwargs['callback'])
</code></pre>

<p>You can then use them in gen.Task:</p>

<pre><code>yield gen.Task(
            lambda **kwargs: db.get_item('users', {"HashKeyElement": {"S": username}}, kwargs['callback']))
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Open File From Console Python Traceback in Text Editor]]></title>
    <link href="http://bbarrows.github.com/blog/2013/01/27/sublimeconsole/"/>
    <updated>2013-01-27T12:48:00-08:00</updated>
    <id>http://bbarrows.github.com/blog/2013/01/27/sublimeconsole</id>
    <content type="html"><![CDATA[<p>Often when I am programming I want to be able to quickly find a function/class definition when I hit a traceback. My normal dev environment is basically just Sublime Text 2 and the OSX console. My Co-Worker Brahm Cohan figured out a cool trick the other day from one of his friends I believe.</p>

<p>Basically, you:</p>

<p>Open, the Automator app. Create a new &ldquo;Service&rdquo;.</p>

<p><img src="http://bbarrows.github.com/images/automator.png" alt="Service Project in Automator" /></p>

<p>The service should execute the following (you can replace Sublime with whatever editor you use):</p>

<p>open -a Sublime\ Text\ 2 $1</p>

<p><img src="http://bbarrows.github.com/images/service.png" alt="Sublime Text command" /></p>

<p>Then open up System Preferences. Goto Keyboard under the Keyboard Shortcuts menu scroll down to find your new service. I named mine tosublime and set the Command-L combo to run it.</p>

<p><img src="http://bbarrows.github.com/images/preferences.png" alt="Perferences Keyboard window" /></p>

<p>Now I can select the absolute path of a file in my python tracebacks and hit Command L. This opens up the file I need to start debugging in. With a little more work I am sure you could parse out the line number too.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Unlimited Vacation]]></title>
    <link href="http://bbarrows.github.com/blog/2013/01/26/vacation/"/>
    <updated>2013-01-26T12:48:00-08:00</updated>
    <id>http://bbarrows.github.com/blog/2013/01/26/vacation</id>
    <content type="html"><![CDATA[<p>I always laugh when I see jobs with unlimited vacation time. This seems like a joke to me. At any company I have ever worked for it seemed like people had vacation time rolling over from previous years or were in fear of losing it for not using it. There seems to be a guilt about using vacation time.</p>

<p>I personally am always worried about using mine, I have this illogical guilt. I feel like I needed to be around working as hard as possible as often as possible or else I will be viewed as a slacker. If anything having an alloted amount of vacation time is a blessing. It represents a company wide accepted amount of time for me to take off. If somone gave me &ldquo;unlimited vacation time&rdquo; I would never take any vacation because I would always feel like I was taking advantage, or at least like I was being percieved as un involved. It makes me wonder if companies realize this and use this &ldquo;benefit&rdquo; to their advantage.</p>

<p>I would love to hear from anyone working at a company with this benefit and hear about the politics behind using it.</p>

<h2>Points from comments here and hacker news</h2>


<p>It was great to see so many different points of view in the comments both here and on Hacker News. After reading a few I felt I should clairify my reasoning a little.</p>

<p>The &ldquo;Unlimited Vacation&rdquo; benefit seems to be more advantageous for companies and possibly misleading for the employees. This [article])(<a href="http://finance.yahoo.com/blogs/the-exchange/unlimited-vacation-time-ultimate-benefit-160807503.html">http://finance.yahoo.com/blogs/the-exchange/unlimited-vacation-time-ultimate-benefit-160807503.html</a>) makes the claim that with the unlimited vacation benefit the limiting factor is really &ldquo;can you get your work done?&rdquo; The ironic thing is that in any smaller tech company, which are the companies I see offering this, there is always work to be done. There has never been a point in the last two years where I personally felt like it was a good time to take a vacation. There has always been some feature we imminently needed. Point being, theres always work to be done so there&rsquo;s rarely a good time to take advantage of your &ldquo;unlimited vacation&rdquo;.</p>

<p>It also saves the company a full paycheck or so when the employee leaves. As anemitz (a Hacker News commenter) pointed out:</p>

<p>&ldquo;Another less likely employer benefit to be considered is that in roughly half of that U.S. states, employers must pay out accrued vacation time if there is a policy in place.</p>

<p>An example from California&rsquo;s vacation faq (<a href="http://www.dir.ca.gov/dlse/faq_vacation.htm">http://www.dir.ca.gov/dlse/faq_vacation.htm</a>):</p>

<p>&lsquo;For example, an employee who is entitled to three weeks of annual vacation (15 work days entitlement per year x 8 hours/day = 120 hours vacation entitlement per year) who quits on August 7, 2002 (the 219th day of the year) without having taken any vacation in 2002, who has no vacation carry-over from prior years, and whose final rate of pay is $13.00 per hour, would be entitled to $936.00 vacation pay upon separation&rsquo;
&#8221;</p>

<p>By giving employees unlimited vacation time and not giving them an alloted amount of PTO it seems like the company won&rsquo;t have to keep track and pay the employee for their remaining PTO when they leave.</p>

<p>And again, my guilt about taking a vacation. Personally, if I do not have a quantified amount of expected vacation time I will not know how much I am expected to take and probably wont take much at all.</p>

<p>On the other hand, as dvo pointed out:</p>

<p>&ldquo;
It might be a great policy, and I&rsquo;m sure it depends on the details of how it is implemented and the culture of the company where it is implemented&hellip;
&rdquo;</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Surf Journal]]></title>
    <link href="http://bbarrows.github.com/blog/2012/12/14/surfjournal/"/>
    <updated>2012-12-14T19:48:00-08:00</updated>
    <id>http://bbarrows.github.com/blog/2012/12/14/surfjournal</id>
    <content type="html"><![CDATA[<p>I think there is a great opportunity to take advantage of all the great NOAA information out there provided by the government for free. With this I can use supervised learning with input from professionals and surfers using the site. I would gather professional input from the other surf forecasting sites out there and allow members to sign up and chart their own surf journals.</p>

<p>This would build surf journals for not just the popular spots like those you see on surfline but for anyspot a surfer desires. He could specify the closest booies and which area the secret spot was in and I would gather wind, tide, and swell information mostly to build a profile of the spot.</p>

<p>This blog post is actually being used as a landing page to gauge interest via a Google AdWords campaign. I will set this as the destination and hope people click through my ads. If so I know there are people interested in this idea as well.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Random Idea About Language Implementation and Strings]]></title>
    <link href="http://bbarrows.github.com/blog/2012/10/29/string-idea/"/>
    <updated>2012-10-29T22:30:00-07:00</updated>
    <id>http://bbarrows.github.com/blog/2012/10/29/string-idea</id>
    <content type="html"><![CDATA[<p>I was reading this awesome post about why this guy <a href="http://www.evanmiller.org/why-i-program-in-erlang.html">appreciates erlang</a>.</p>

<p>And this paragraph caught my eye.</p>

<hr />

<p>Or take string concatenation. If you pop open the implementation of string concatenation in Perl, Ruby, or JavaScript, you&rsquo;ll are certain to find an if statement, a realloc, and a memcpy. That is, when you concatenate two strings, the first string is grown to make room for the second, and then the second is copied into the first. This approach has worked for decades and is the “obvious” thing to do. Erlang&rsquo;s approach is non-obvious, and, I believe, correct. Erlang does not use a contiguous chunk of memory to represent a sequence of bytes. Instead, it represents a sequence of bytes as nested lists of non-contiguous chunks of memory. The result is that concatenating two strings takes O(1) time in Erlang, compared O(N) time in other languages. This is why template rendering in Ruby, Python, etc. is slow, but very fast in Erlang.</p>

<hr />

<p>I just thought it would be a cool little mini project to try and re implement Python and or Ruby strings to act like Erlang strings (the linked list of bytes versus contiguous). I am kind of using this as a note to myself since writing things down in other places just gets lost a lot of the time..</p>

<p>It would be cool to do this though and then compare the new ruby build with the previous in a bunch of different speed string tests.</p>

<p>After talking to some friends at work I realized that this already done. Using the rope data structure. A cool implementation, <a href="https://github.com/josephg/librope">librope</a> is a very interesting read.</p>

<p>After checking the librope implementation out I realized that using these strings would only be a benfit in very specific situations, long modifiable strings.</p>

<p>However, having an implementation accessbile in a standard library would deff be a cool thing to have. I could see this being super helpful when doing something like building a website response from templates.</p>

<p>Actually from a quick google it sounds like this is how PyPy implemented strings.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Github Diary]]></title>
    <link href="http://bbarrows.github.com/blog/2012/10/08/github-diary/"/>
    <updated>2012-10-08T15:54:00-07:00</updated>
    <id>http://bbarrows.github.com/blog/2012/10/08/github-diary</id>
    <content type="html"><![CDATA[<p>Every morning I have to present the tasks I have been working on in a meeting called Scrum. This is helpful for a number of reasons. It keeps everyone focused, the team knows what is going on and who to talk to about specific issues, keeps people from working on the same problem, etc..</p>

<p>One thing that bugs me though is that I can never remember the full list of things I have been working on. To fix this I started making a list of my daily activities in a file called WhatIDid on my Desktop. This actually worked really well.</p>

<p>Then recently I needed to give my boss a list of all the project I had been working on recently. This was really hard to do as I have done a ton of work on about every part of our projects architecure. Thats when I decided I would use Github as my diary.</p>

<p>I now commit my WhatIDid file in a private repository with the date as the commit message. This way I can browse back through time whenever I want and see all the work I have accomplished when necessary.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Updating the Ami of an EC2 AutoScaling Group]]></title>
    <link href="http://bbarrows.github.com/blog/2012/09/26/updating-the-ami-of-an-ec2-autoscaling-group/"/>
    <updated>2012-09-26T11:51:00-07:00</updated>
    <id>http://bbarrows.github.com/blog/2012/09/26/updating-the-ami-of-an-ec2-autoscaling-group</id>
    <content type="html"><![CDATA[<p>The other day I noticed that some our instances were coming up with package install issues but when I ran the script on a fresh test instance everything worked fine.</p>

<p>After looking into the differences between my test instance and the autoscaling group instances I noticed that the AMI (Operating system) was different.</p>

<p>In order to change this I needed to get the <a href="http://aws.amazon.com/developertools/2535">AutoScaling Group Command Line Tools</a> and do a number of things.</p>

<p>First I needed to create a new launch config. A launch config tells an autoscaling group what size EC2 instance to run and what ami to use. In order to see how your current launch config is configured you can run:</p>

<pre><code>./as-describe-launch-configs your-launch-config --show-long
</code></pre>

<p>I then noticed my AMI was an old Debian image.</p>

<p>I wanted to use Ubuntu for this service. I ran:</p>

<pre><code>./as-create-launch-config your-new-launch-config --image-id ami-3c994355 --instance-type m1.small --key post-svn-rebuild --user-data-file ~/user-data-bootup-script.sh
</code></pre>

<p>your-new-launch-config is the name of the launch config I am creating.
~/user-data-bootup-script.sh Is the script that is ran when a new machine is brought up. We use this to bootstrap our instance deployment scripts.</p>

<p>I then updated our autoscaling group to use the new launch config and killed off the old instance (creating a new instance to take its place)</p>

<pre><code>./as-update-auto-scaling-group your-autoscaling-group --launch-configuration your-new-launch-config

./as-terminate-instance-in-auto-scaling-group i-894ffff4 --no-decrement-desired-capacity
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Python Min Heap and Comparators]]></title>
    <link href="http://bbarrows.github.com/blog/2012/09/24/python-min-heap-and-comparators/"/>
    <updated>2012-09-24T23:22:00-07:00</updated>
    <id>http://bbarrows.github.com/blog/2012/09/24/python-min-heap-and-comparators</id>
    <content type="html"><![CDATA[<p>You know what really grinds my gears?!? Not being able to easily create a min heap in Python 2.7.</p>

<p>Actually its not that bad. But it would be nice if you could specify a Comparator or something similar like Java. For example, check the <a href="http://docs.oracle.com/javase/1.4.2/docs/api/java/util/TreeSet.html" title="Java TreeSet Class">TreeSet</a> Class. The constructor takes an object the implements the Comparator interface which means that it acts like the Python magic method <a href="http://www.rafekettler.com/magicmethods.html"><strong>cmp</strong></a>.</p>

<p>This is awesome because you can easily specify how you want to compare the objects in your heap. Normally you probably just want to compare objects against one another. This makes sense in 90% of situations. However, in my super awesome DHT project I needed to keep a list of the top N number of nodes I had recieved from my queries in order. The order I needed was from smallest to largest DHT Distance deom myself to them.</p>

<p>In the <a href="http://en.wikipedia.org/wiki/Kademlia" title="Kademlia DHT">Kademlia DHT</a> the distance between two nodes is calculated by XORing each ndoes id. Therefore, to find which of two nodes is closest to me I need to compare each nodes id after xoring it with my own.</p>

<p>The first idea that comes to mind is to just override my DHTPeer objects cmp method however that wouldn&rsquo;t make sense in any other sitaution when I wanted to comapre two peers.</p>

<p>In order to get around this I settled with using tuples.</p>

<p>I simply heappush a tuple of the metric (DHT distance) I want ot sort on and the object I really want to keep track of in the first place. This way I don&rsquo;t need to mess with the object I want sorted in my heap and I can sort whichever way I want.</p>

<p>Here is an example of how I am using it:</p>

<pre><code>#Part of a class from my project
class NodeListHeap(object):
    CONTACT_LIST_LENGTH = 5
    def __init__(self, dht_node_id):
        self.dht_node_id = dht_node_id
        self.node_heap = []

    def push(self, dht_peer):
        heappush(self.node_heap, (dht_dist(self.dht_node_id, dht_peer.id), dht_peer))
</code></pre>

<p>The first argument of heappush is the heap you are pushing on and then the second is the value. Since the comparison of tuple first compares the first value in the tuple the dht_dist becomes the value that is being compared for the value I want to store(the dht_peer object).</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Python String Bit Iterator]]></title>
    <link href="http://bbarrows.github.com/blog/2012/09/24/python-string-bit-iterator/"/>
    <updated>2012-09-24T22:55:00-07:00</updated>
    <id>http://bbarrows.github.com/blog/2012/09/24/python-string-bit-iterator</id>
    <content type="html"><![CDATA[<p>My most recent project has been to implement a BitTorrent client (or maybe just part of one) and a <a href="http://en.wikipedia.org/wiki/Kademlia" title="Kademlia DHT">DHT</a> client! Really the BitTorrent client is just to support the DHT client. Anyhow, I found that in my DHT implementation re occuringly needed to take a string of bytes and iterate bit by bit through it accomplishing some task along the way.</p>

<p>Now in Python 3 there are all kinds of cool tools to do this kind of thing but I am working in Python 2.7 . To accomplish my goal I used yield to create a generator iterator of a python string, that iterates bit by bit.</p>

<pre><code>#Returns a iterator that will iterate bit by bit over a string!
def string_bit_iterator(str_to_iterate):
    bitmask = 128 #1 &lt;&lt; 7 or '0b10000000'
    cur_char_index = 0

    while cur_char_index &lt; len(str_to_iterate):
        if bitmask &amp; ord(str_to_iterate[cur_char_index]):
            yield 1
        else:
            yield 0

        bitmask = bitmask &gt;&gt; 1
        if bitmask == 0:
            bitmask = 128 #1 &lt;&lt; 7 or '0b10000000'
            cur_char_index = cur_char_index + 1
</code></pre>

<p>This is pretty self explanatory I hope. It just starts at the first byte in the string, with a bitmask with the most important big set and begins shifting the bit right, reseting and moving onto the next byte when needed.</p>

<p>Heres some of the tests I wrote for this:</p>

<pre><code>bit_iter = string_bit_iterator('\xff\xff\xff\xff\xff\xff\xff\xff')
for b in bit_iter:
    assert(b == 1)

count = 0
last = 0
bit_iter = dhttornado.string_bit_iterator('\xaa') #0xAA is b10101010
for b in bit_iter:
    assert(b != last)
    last = b
    count = count + 1
assert(count == 8)

peer = new_node([0xAA,0xAA])
for b in peer:
    assert(b != last)
    last = b
</code></pre>

<p>This was used for iterating over the bits in an info hash or 20 byte peer ID in my DHT implementation (which is on my github).</p>

<p>Hopefully this takes care of some bitmasking/shifting for someone else..</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Installing OpenCL and PyOpenCL on Ubuntu 12.10]]></title>
    <link href="http://bbarrows.github.com/blog/2012/09/24/opencl-on-ubuntu-1210/"/>
    <updated>2012-09-24T22:47:00-07:00</updated>
    <id>http://bbarrows.github.com/blog/2012/09/24/opencl-on-ubuntu-1210</id>
    <content type="html"><![CDATA[<h1>The hassle of Intel OpenCL Dev</h1>

<h2>Trying to mine some damn bitcoin</h2>

<p>Fueled by the ridiculous BitCoin bubble and yes, I know, late to the game I decided I might as well start a bitcoin miner on an extra computer or two.</p>

<p>What I didnt expect is the huge pain in the ass that was installing the Intel OpenCL libs/headers..</p>

<p>Ill try to sum this up as quickly as possible:</p>

<p>I started with <a href="http://develnoter.blogspot.com/2012/05/installing-opencl-in-ubuntu-1204.html">this guys blog entry on installing OpenCL on ubuntu</a> which</p>

<p>This is directly from his site, slightly modified:</p>

<hr />

<ol>
<li>Download the <a href="http://software.intel.com/en-us/articles/vcsource-tools-opencl-sdk/">Intel SDK for OpenCL Applications from Intel&rsquo;s web site</a>. The download options are not easy to spot in the messy page, they are on the top right bo. After downloading, you will end with a .tgz file with an RPM inside (crazy, I know).</li>
</ol>


<p>The guy who I stole this walk through from says he got the file :
    intel_sdk_for_ocl_applications_2012_x64.tgz.
NOTE!! I actually didnt see this option I downloaded the option with all the check marks for the beta that supported RedHat Linux. I got this file:
    intel_sdk_for_ocl_applications_2013_xe_beta_sdk_3.0.56860_x64.tgz</p>

<ol>
<li>Extract the RPM from the .tgz file:
 $ tar zxvf intel_sdk_for_ocl_applications_2012_x64.tgz</li>
</ol>


<p>This will extract the intel_ocl_sdk_2012_x64.rpm file.</p>

<ol>
<li>Convert the RPM file to .deb format and install:
 $ sudo apt-get install -y rpm alien libnuma1    # In case you don&rsquo;t have these packages
 $ fakeroot alien &ndash;to-deb intel_ocl_sdk_2012_x64.rpm</li>
</ol>


<p>The guy says to run:</p>

<pre><code>$ sudo dpkg -i intel-ocl-sdk_2.0-31361_amd64.deb
</code></pre>

<p>However you will probably have 4 diff debs. I just ran the following to install them all:
sudo dpkg -i *.deb</p>

<ol>
<li>Now the SDK and libraries will be installed to /usr/lib64, while Ubuntu expects them to be in /usr/lib. No problem, just make a symlink and update the library cache:
 $ sudo ln -s /usr/lib64/libOpenCL.so /usr/lib/libOpenCL.so
 $ sudo ldconfig</li>
</ol>


<p>That&rsquo;s it! OpenCL should be installed now. Let&rsquo;s try with a test program to see the device capabilities.</p>

<pre><code>#include &lt;iostream&gt;
#include &lt;CL/cl.hpp&gt;
#include &lt;boost/foreach.hpp&gt;

int main(int, char**) {
 std::vector&lt;cl::Platform&gt; platforms;
 cl::Platform::get(&amp;platforms);
 BOOST_FOREACH(cl::Platform platform, platforms) {
     std::cout &lt;&lt; "Platform: " &lt;&lt; platform.getInfo&lt;CL_PLATFORM_NAME&gt;() &lt;&lt; std::endl;
     std::vector&lt;cl::Device&gt; devices;
     platform.getDevices(CL_DEVICE_TYPE_GPU | CL_DEVICE_TYPE_CPU, &amp;devices);
     BOOST_FOREACH(cl::Device device, devices) {
         std::cout &lt;&lt; "Device: " &lt;&lt; device.getInfo&lt;CL_DEVICE_TYPE&gt;();
         std::cout &lt;&lt; " (" &lt;&lt; CL_DEVICE_TYPE_GPU &lt;&lt; " means GPU, " &lt;&lt; CL_DEVICE_TYPE_CPU &lt;&lt; " means CPU)" &lt;&lt; std::endl;
     }
 }
}
</code></pre>

<p>Save this snippet into opencl.cpp, compile and run:
$ g++ opencl.cpp -lOpenCL -o opencl &amp;&amp; ./opencl
Platform: Intel&reg; OpenCL
Device: 2 (4 means GPU, 2 means CPU)
This shows that this particular machine has one OpenCL-capable Intel CPU.</p>

<hr />

<p>So continuing on I basically kep trying to compile his example until it succesfully compiled and ran.</p>

<p>So along with installing the libs I needed the headers to do this I made the following dir:</p>

<pre><code>sudo mkdir -p /usr/local/cuda/include/CL
cd /usr/local/cuda/include/CL
sudo wget http://www.khronos.org/registry/cl/api/1.1/cl.hpp
sudo wget http://www.khronos.org/registry/cl/api/1.1/cl.h
sudo wget http://www.khronos.org/registry/cl/api/1.1/opencl.h
sudo wget http://www.khronos.org/registry/cl/api/1.1/cl_platform.h
sudo wget http://www.khronos.org/registry/cl/api/1.1/cl_gl.h
sudo wget http://www.khronos.org/registry/cl/api/1.1/cl_gl_ext.h
sudo wget http://www.khronos.org/registry/cl/api/1.1/cl_ext.h
sudo chmod -R a+r /usr/local/cuda
</code></pre>

<p>If you are missing any other headers try looking for the file under the
    <a href="http://www.khronos.org/registry/cl/api/1.1/">http://www.khronos.org/registry/cl/api/1.1/</a>
directory..</p>

<p>I also installed:</p>

<p>sudo apt-get install libboost-all-dev python-setuptools python-numpy</p>

<p>I actually installed numpy with pip but that should work too..</p>

<p>I downloaded <a href="https://pypi.python.org/pypi/pyopencl">PyOpenCL</a> and ran
    sudo python setup.py install
however it couldnt find the headers.</p>

<p>I actually just added the following line to setup.py, right before the line:
    INCLUDE_DIRS = conf[&ldquo;BOOST_INC_DIR&rdquo;] + conf[&ldquo;CL_INC_DIR&rdquo;]
I added:
    conf[&ldquo;CL_INC_DIR&rdquo;] = [&lsquo;/usr/local/cuda/include/&rsquo;, &lsquo;/usr/local/cuda/include/CL&rsquo;]</p>

<p>althouh you prob only need that first one..</p>

<p>then running:
    sudo python setup.py install
worked great..</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Implementing Exception Logging in Python]]></title>
    <link href="http://bbarrows.github.com/blog/2012/09/24/implementing-exception-logging-in-python/"/>
    <updated>2012-09-24T22:47:00-07:00</updated>
    <id>http://bbarrows.github.com/blog/2012/09/24/implementing-exception-logging-in-python</id>
    <content type="html"><![CDATA[<p>
When tasked with logging all the exceptions that our software might encounter in the wild I tried a number of different techniques to log our python exception tracebacks. I ended up with an application that would take logs of a specific level and upload them to a server (in our case Loggly) where we could analyze them.
</p>




<p> The first hurdle was determing how I could install my own method to process and log ALL tracebacks happening in the application.
</p>




<p>
At first I thought that the python system exception hook, <a href="http://docs.python.org/library/sys.html">sys.excepthook</a> would be the perfect place to insert the logging code. I was trying something similar to:
</p>


<pre>
import traceback
import StringIO
import logging
import os, sys

def my_excepthook(excType, excValue, traceback, logger=logger):
    logger.error("Logging an uncaught exception", exc_info=(excType, excValue, traceback))

sys.excepthook = my_excepthook
</pre>


<p>
This worked for the main thread but I soon found that the my sys.excepthook would not exist across any new threads my process started. This is a huge issue because most everything happens in threads in this project.
</p>


<p>
After googling and reading plenty of documentation the most helpful information I found was from the <a href="http://bugs.python.org/issue1230540">Python Issue tracker</a>.
</p>


<p>
The first post on the thread shows a working example of the sys.excepthook NOT persisting across threads (as shown below). Apparently this is expected behavior.
</p>


<pre>
import sys, threading

def log_exception(*args):
    print 'got exception %s' % (args,)
sys.excepthook = log_exception

def foo():
    a = 1 / 0
threading.Thread(target=foo).start()
</pre>




<p>The messages on this Python Issue thread really result in 2 suggested hacks. Either subclass Thread and wrap the run method in our own try except block in order to catch and log exceptions or monkey patch threading.Thread.run to run in your own try except block and log the exceptions.</p>


<p>The first method of subclassing Thread seems to me to be less elegant in your code as you would have to import and use your custom Thread class EVERYWHERE you wanted to have a logging thread. This ended up being a hassle because I had to search our entire code base and replace all normal Threads with this custom Thread. However, it was clear as to what this Thread was doing and would be easier for someone to diagnose and debug if something went wrong with the custom logging code. A custom logging thread might look like this: </p>




<pre>
class TracebackLoggingThread(threading.Thread):
    def run(self):
        try:
            super(TracebackLoggingThread, self).run()
        except (KeyboardInterrupt, SystemExit):
            raise
        except Exception, e:
            logger = logging.getLogger('')
            logger.exception("Logging an uncaught exception")
</pre>




<p>The second method of monkey patching threading.Thread.run is nice because I could just run it once right after __main__ and instrument my logging code in all exceptions. Monkey patching can be annoying to debug though as it changes the expected functionality without being obvious that this was done. The suggested patch from the Python Issue tracker was: </p>




<pre>
def installThreadExcepthook():
    """
    Workaround for sys.excepthook thread bug
    From
http://spyced.blogspot.com/2007/06/workaround-for-sysexcepthook-bug.html

    Call once from __main__ before creating any threads.
    If using psyco, call psyco.cannotcompile(threading.Thread.run)
    since this replaces a new-style class method.
    """
    init_old = threading.Thread.__init__
    def init(self, *args, **kwargs):
        init_old(self, *args, **kwargs)
        run_old = self.run
        def run_with_except_hook(*args, **kw):
            try:
                run_old(*args, **kw)
            except (KeyboardInterrupt, SystemExit):
                raise
            except:
                sys.excepthook(*sys.exc_info())
        self.run = run_with_except_hook
    threading.Thread.__init__ = init
</pre>




<p>The main goal of creating somethign that could log any of our tracebacks was to send these tracebacks up a server where we could search through issues people were having in the wild and find bugs in our software.
    It was not until I started testing my exception logging I realized that I was going about it all wrong.</p>




<p>To test I had placed a <pre>raise Exception("Test")</pre> and/or a <pre>1/0</pre> somewhere in my code. However, wrapping a a method that called this method was a try except block that printed out the traceback and swallowed the exception. This was very frustrating because I saw the traceback bring printed using logging.error but not being processed by my custom exception handler. My exception handlers purpose was to take these tracebacks and ultimately upload them to a server. </p>




<p>The final implementation consisted of a few different classes that looked something like this:</p>




<pre>
class CustomLoggingfThread(Thread):
    def __init__(self, daemon_thread=False, group=None, target=None, name="LiveThread",
                 args=(), kwargs={}, verbose=None, clean_up=None):
        self.args = args
        self.kwargs = kwargs

        # This will make sure that the thread is wrapped in the try except block
        #regardless of whather or not you extend the class are use the target invocation
        #of Thread
        if type(self) != LiveThread:
            assert hasattr(self,"run")
            self.target = self.run
        else:
            self.target = target
        Thread.__init__(self, group=group, target=target, name=name,
                 args=args, kwargs=kwargs, verbose=verbose)
        self.run = self.__run
        self.daemon = daemon_thread


    def __run(self):
        try:
            self.target(*self.args, **self.kwargs)
        except Exception:
            logging.error("The thread %s raised an exception." % self.name, exc_info=True)
            #We consider exiting our application here if its an important thread that crashed and can't
            #be restarted.

</pre>




<p>
As you can see, our custom logging thread became VERY simple. It just wraps the threaded function in a try except block and logs any tracebacks to logging.error
</p>




<p>This class is meant to be used in combination with our custom logging filter and handlers. The python Logging module provides some great tools to help you control what you do with your logging. My implementation looked something like this:
</p>




<pre>


class CustomLoggingFilter(logging.Filter):
    def filter(self, record):
        return record.levelno >= logging.WARNING

class CustomLogglyHttpHandler(hoover.LogglyHttpHandler):

...
    Removed some code to be brief..
...

    def emit(self, record):
        if isinstance(record.msg, (list, dict)):
            record.msg = dumps(record.msg, cls=self.json_class, default=str)
        msg = self.format(record)
        self.synchronous_loggly_post(msg)



loggly_handler = CustomLogglyHttpHandler(.....some args....)
loggly_handler.addFilter(CustomLoggingFilter())

#Get the root logger so all log msgs run through our handler/filter
loggly_logger = logging.getLogger('')
loggly_logger.addHandler(loggly_handler)

</pre>




<p>
The above code is really what puts this all together. It adds a logging handler to post only the log messages of a certian log level (via a filter) to the server.

In the actuall implementation we do a few things such as buffering of the log messages as well. These were left out to keep the post short(er).
</p>

]]></content>
  </entry>
  
</feed>
